---
title: 分布式系统-6.824知识总结
date: 2022-03-10 16:22:25
tags: 
    - Distributed System
    - 面试
    - 项目
mathjax: true
categories: 项目
---

# 1 基本概念

- 什么是分布式系统？

- 为什么要搭建分布式系统？

  通过并发提高**容量**

  通过复制提供**容错性**

  通过隔离实现**安全性**

- 为什么说分布式系统的**容错性**、**一致性**、**性能**之间存在冲突？

  为了保证分布式系统的容错性、数据需要在多台服务器上复制；为了保证分布式系统的一致性，需要在多台服务器上进行通信；但数据的复制和大量通信都会影响分布式系统的性能

# 2 谷歌文件系统

## 谷歌文件系统的需求

- **节点故障是常态**：面对大概率发生的节点故障问题，因为分布式系统构建在大量普通机器上，因此所有机器都不失效的几率较低。这要求GFS必须具有容错性，且必须具有使故障节点快速恢复的机制
- **以大文件为主**：所存储的文件通常是几GB级别的
- **主要负载**：大容量连续读、小容量随机读、大容量追加写（几乎没有随机写，数据一旦写入便不再修改）
- 支持高效且原子性的**文件追加操作**
- **高吞吐量**比**低延时**更重要

## GFS集群组成

- **1 Master + x Chunk servers**

  GFS集群包括一个**Master节点**和若干个**Chunk Server**（Chunk=块），作为用户及进程运行在普通的Linux机器上

- **文件切分为chunk，replica存储在多台chunk server**

  存储文件时，GFS会把文件切分成若干个拥有固定长度的chunk并存储。Master在创建chunk时会为它们赋予一个唯一的64位Handle，并把它分配给Chunk Server。Chunk server以普通文件的形式将每个Chunk存储在自己的本地硬盘上，位确保Chunk的可用性，GFS会把每个Chunk备份成若干个Replica，分配到Chunk Server上

- **Master的工作**

  - **维护文件系统元数据**：元信息包括命名空间、访问控制、从文件到chunks的映射，每个chunk的位置
  - **通过心跳信号定期与Chunk servers通信**：向其发送指令，并获取chunk servers当前的状态信息

## GFS的元数据

- GFS集群的**元数据**

  - 文件与Chunk的命名空间

  - 文件与Chunk的映射关系

  - 每个Chunk replica所在的位置

- **先写日志**
  - Master对元数据做任何操作前会用**先写日志**的形式将操作进行记录，日志写入完成后再进行实际操作
  - 日志也被备份到多个机器上保存

## GFS读写文件

### 读文件

- **客户发送文件名和偏移量**：客户得到文件名和偏移量，发送请求给master
- **master返回chunk对应的服务器列表**：master根据文件名确定chunk ID array，根据offset确定为那个元素，得到chunk ID后可查询到存储chunk的server list，并将其返回client
- **客户向其中一个服务器发送chunk ID和偏移量**：client从server list中选取一个server（从最近的开始），将chunk ID和offset发送给chunk-server
- **server返回数据**：服务器收到请求后，从磁盘读取数据并返回给client

### 写文件

- **客户向master查询文件最后一个chunk**：客户向master发送文件名，查询该文件的最后一个chunk (追加操作)
- **master更新primary chunk**: master查看该文件是否指定primary chunk，没有则：
  - **选择primary chunk**：从各个replica选取最新的一个作为primary replica，其余作为secondary replica
  - **更新chunk版本号**：master更新chunk的版本号并写入disk
  - **告知各个server其身份**：master向各个server发送消息告知其primary/secondary身份及新版本号
  - **租约**：对primary赋予一个租约
- **master告知客户chunk server**: master向server返回primary和secondary chunk所在的server
- **客户向所有server发送数据**：这些数据放入临时位置，不会直接追加到文件中
- **客户向primary服务器发送消息**：告知primary所有该chunk的服务器都有了需要追加的数据
- **primary服务器发送追加通知**：若primary chunk所在server有足够空间，则向所有secondary server发送通知让其追加数据
- **secondary服务器告知追加结果**：secondary服务器告知primary服务器是否写入成功
- **primary服务器告知客户追加结果**：若所有secondary服务器都写入成功，则告知客户写入成功

## 数据一致性

- **命名空间的数据一致性**：命名空间由单节点Master管理在其内存中，通过**互斥锁**解决并发修改问题
- **不保证强一致性**：GFS不保证强一致性，从同一个chunk的不同replicas读取到不同数据是正常的

- **GFS对写失败的处理**：此时各个replicas的状态可能不一致，GFS允许这种情况发生

### Master无法联系primary服务器

- **Ping primary**: Master向primary服务器发送租约后，定期Ping primary
- **无法收到回复**：可能是primary故障，也可能是master与primary之间的通信问题，如**网络分区**等
- **等待租约时间过期**：若master直接分配下一个primary，则可能同时出现两个primary（网络分区），因此它会等待该primary的租约时间过期后再分配primary

## 系统操作

### 文件追加

- **至少一次**：GFS保证Append至少成功执行一次
- **客户失败后重新执行**：客户收到Master通知的写失败后，将再次发起Append操作直至写入成功，因此不同的chunk server可能包含不同次数的该Append数据

### 文件快照

- 作用：为指定的文件或目录创建副本

- **COW技术**

  > copy-on-write fork技术：当父进程使用fork()创建子进程时，先不给子进程分配物理页面，而是让子进程页表中的指针指向父进程对应的物理页面，即父子进程共享同一个页面。当父子进程中的任意一个写入该页面时，会引发page fault，并陷入内核中，内核为该进程分配一个物理页面，并复制数据，修改其页表的相应项使其指向该新分配页面，并置相关标志位，此后父子进程便可以正常对各自的页面进行读写

  - **撤回租约**：Master收到快照请求后，撤回这些chunk的租约，因此该文件的chunk不存在primary服务器
  - **master复制**：租约撤回后，master写日志，对自己管理的命名空间进行复制操作，产生的新纪录指向原本的chunk
  - **客户写入**：此后客户写入该文件的chunk，必须先向master发送请求获取primary服务器的位置
  - **写时复制**：master注意到该chunk的引用计数大于1，因此master会通知所有持有该chunk的chunk server在本地复制一个新的chunk，并应用新的handle，返回给客户端

# 3 MapReduce

## 基本概念

- **输入划分**：将输入划分为多份，对于每份输入调用一个`Map()`，产生中间值
- **收集中间值**：将每个key及其对应的值传入一个`Reduce()`
- **输出结果**：从多个`Reduce()`返回的是输出结果

## 分区函数

- **Map per input file**: MapReduce通常为每个输入文件执行一个Map
- **R output files**: MapReduce使用者一般指定Reduce任务输出文件的数量R
- **R Reduces**: 常用方法是使用$hash(key)\%R$将Reduce任务划分为R份

## 可拓展性

- **并发**：`Map()`和`Reduce()`都可以并发，不同worker上的`Map()`和`Reduce()`不会相互影响
- N workers意味着N倍吞吐量

## 数据存储

- **GFS**：输入和输出都放在**谷歌文件系统**

## MapReduce需要通过网络传输数据

- **Map读取数据**：从GFS读取数据传入`Map()`
- **Map发送中间值给Reduce**：从`Map()`读取中间值送入`Reduce()`
- **Reduce结果写回GFS**：`Reduce()`产生的结果写回GFS

## MapReduce框架

- **1 coordinator**：负责将map tasks发送给workers
- **中间值的存储**：Map输出的中间数据写入本地磁盘，并按hash值划分，每个hash值写入一个disk
- **输出文件**：一个Reduce worker从本地磁盘获取具有具有某个hash值的所有中间数据，处理后将输出存放在一个output file，并将该file写回到GFS

## MR如何减少网络使用

- **server同时担任多角色**：每个server同时运行GFS和Map worker、Reduce worker
- **就地Map**：尽量在存放输入文件的GFS server上执行Map

## 负载均衡问题

- **增加tasks数量**：让tasks数量远多于workers，减小每个任务的大小
- **coordinator分配任务**：将任务交给已经完成先前任务的空闲worker

## 容错问题

- **重新执行**：某个worker故障后，重新执行其上的Map或Reduce任务即可，对其他任务无影响
- **Map和Reduce是完全确定函数**：如果某个Map任务执行了两次，部分Reduce worker获取了第一次的结果，部分获取了第二次的结果，则完全确定函数保证两次的结果是一致的
- **故障恢复**：coordinator发现故障worker，获取其任务，并将它分配给其他worker重新执行

## 其他问题

- 为什么Map会执行多次？

  Coordinator由于网络通信问题无法联系某个Map worker，并把其任务分配给其他Map worker，但实际上该Map worker并未故障且成功执行任务，从而导致了Map执行了两次

- **Map执行多次**会出现问题吗？

  不会，Map的确定函数性质保证了多次的执行结果是一致的，且Coordinator保证只会给每个Reduce worker分配至多一次的Map结果

- **Reduce执行多次**会出现问题吗？

  Reduce的确定函数性质保证了输出结果的一致性，Reduce worker尝试将相同的输出文件写入GFS，而GFS机制保证最终只有一个文件存在(命名机制，相同的Reduce任务的输出文件名是相同的)

- 如果某个worker**速度特别慢**会怎样？
  - **分配未完成部分给空闲worker**: coordinator将该worker最后的进行中任务分配给其他空闲worker执行，但原来的worker也继续执行
  - **两者之一完成即可**：当原来worker或新分配的worker中的任意一个执行完成，该任务即执行完成

# 4 RPC和线程

## 线程

- 线程的特点

  **共享内存**：线程之间共享内存

  **独立状态**：每个线程拥有一些独立状态，包括程序计数器、栈、寄存器内容

  **协程**：Go中的线程称为协程goroutines

- 为什么需要线程

  **I/O并发**：客户可能同时向许多服务器发送多个请求，一个服务器可能接收到多个不同客户的请求，当客户A的请求需要从磁盘读取数据时，服务器CPU可以无需一直等待，而是切换到另一线程以执行客户B的请求

  **多核-并行**：如果机器的CPU是多核的，则通过将不同线程分配在不同核上运行，实现并行执行任务

  **检查worker是否存活的后台线程**：增加一个线程，专门用于定时向各个worker发送ping信号，检查其是否处于工作状态

## 远程过程调用RPC

- 基本概念

  **客户**：请求服务的进程

  **服务器**：提供服务的进程

  **RPC处理函数**：服务器对于不同类型的请求，需要用不同的RPC处理函数进行响应，各个RPC处理函数的参数和返回值是不同的

- RPC handler的并发性

  **goroutine per request**：对于每个请求，服务器可以创建一个goroutine进行处理

  **共享数据的修改**：RPC处理函数修改共享数据时必须先获取互斥锁

- RPC如何**处理failure**

  - **best effort模型**

    设置**最大等待时间**$t$，与**最多重新发送次数**$c$

    调用`Call()`发送请求后，若等待$t$则重新发送request，并且重传次数加1

    若重传次数达到$c$，则返回error

  - **at most once模型**

    如果从服务端到客户端的网络通信故障，则服务端可能收到来自客户端的多个相同请求，此时不应该重复执行RPC处理函数

    **client ID + XID**：为了标识客户及其请求，每个客户有一个唯一的client ID，其每个请求有一个唯一的XID，通过client ID与XID组成的远二元组能够标识客户的请求

- GO中的RPC模型

  **只发送一次请求**：若固定时间没收到Reply后，则直接返回error

# 5 Raft算法

## Primary结构及其Split brain问题

- **Primary结构**：即系统中存在Primary节点，而其他节点为Secondary节点，Primary节点具有维护系统状态、控制任务执行的职责
- **Split brain问题**：考虑互为replica的两台客户主机C1、C2的情况，两个服务器S1、S2提供Test-And-Set服务保证了某个时刻只有一个主机能成为primary。如果C1与S1同属一个分区，C2与S2同属一个分区，而他们之间的通信网络故障，则要么两个客户同时上线（Split brain），要么两个客户都无法上线（没有容错性）

- **Split brain问题的解决**

  无故障网络：通过维护专用网络保证无故障

  人工维护：若客户无法与S1、S2任意一个通信时，则与机房人员联系并将该服务器关闭，保证不会有split brain问题

- **更好的解决方案Majority Vote**

  primary的决策得到过半数的投票即可执行，无需得到全部服务器投票，从而实现容错

  若系统有2F+1台服务器，则可接受F台服务器故障

## Leader选举

### 新选举的原因

- **leader故障**：leader故障因而不发送心跳信号
- **网络延迟**：由于网络延迟导致节点长时间没收到leader的心跳信号

### 选举计时器

- **超时发起选举**：如果当前节点长时间没收到leader的心跳信号，且定时中断，则当前节点发起选举

- **重启计时器**：节点收到当前leader的心跳信号时，重启计时器

- **Split Vote问题及解决**：如果大多数节点总是同时开始选举，则它们都会先给自己投票，然后再向其他节点获取选票，这样则没有任何节点能够得到过半数选票，并导致选举计时器超时。如果重启计时器，并再次开始选举，则会再次进入同样的情况。为了防止无限的开始选举，Raft在重启计时器时总是**随机选择超时时间**

- **何时发生超时**

  follower长期没有收到leader的心跳信号，超时后开始选举

  candidate长期没有收到过半数选票，也没有收到其他节点成为leader的指令，超时后重新开始选举

- **超时时间的合理取值**

  **随机选择**：以上两个超时发生后，总是随机选择一个超时时间，并重启计时器

  **超时时间**：应该是leader发送心跳信号间隔的几倍，并小于server的平均故障时间

  > - 下限：如果超时时间设置为小于leader发送心跳时间的间隔，则会在leader正常工作的情况下开启新的选举，显然是错误的
  > - 上限：假如leader故障则系统会在一段时间内陷入瘫痪，直至有节点开启新的选举。因此如果故障间隔时间越短，则选举间隔时间也应该越短，以尽量减少系统瘫痪的时间

### 如何确保旧leader行为正确

- **网络分区**：若因为网络分区等原因，在旧leader仍未故障的情况下，产生了新的leader，则旧leader可能无法获知新leader的产生
- **Majority Vote**：由于新leader产生必定获得了过半数节点的投票，因此旧leader的任何指令都无法收到过半数的投票，从而在新leader产生后旧leader无法再执行任何指令

## 选举约束

- **投票**：当candidate向其他节点发送请求投票RPC时，只有当candidate的日志记录至少和自己一样up-to-date时，节点才会给它投票

- **up-to-date的定义**

  如果候选人的last term大于自己的last term

  如果两者的last term一样大，但候选人的last index大于等于自己的last index

## 日志复制

- **日志复制**：Raft通过log同步时序，因此当leader节点决定接收客户请求并执行某操作时，需要添加日志，并在其他节点也复制该日志，等待超过半数节点确认后，再提交日志并在state machine上执行操作

- **添加日志指令**：client向leader发送请求，leader先在本节点添加日志，同时向其他服务器发送添加日志指令
- **leader提交日志、执行、返回结果**：若过半数的服务器确认该指令，则leader提交日志，并让状态机执行该日志的命令，同时将执行结果返回client
- **提交日志指令**：leader还需要向其他服务器发送提交日志指令，一般将“提交日志”指令附在下一次的“添加日志”指令发送

## 日志恢复

- **`NextIndex`变量**：leader对于每个follower，都维护了一个`NextIndex`变量，代表该follower下一个应该匹配的日志记录位置
- **follower拒绝添加日志RPC**：当leader向follower发送添加日志RPC时，让follower复制从`NextIndex`位置到当前日志结尾之间的日志记录，follower则对比`prevLogIndex`上的自己的日志记录与leader的日志记录是否一致，如果不一致则拒绝该请求
- **递减`NextIndex`**：leader若收到follower的拒绝，则代表该follower的`NextIndex`位置之前的日志记录不匹配，因此leader会递减其`NextIndex`，并对比更前面的位置是否匹配，直到遇到匹配的位置
- **匹配成功并添加**：该过程持续直到follower成功匹配`prevLogIndex`上的日志记录，此后接收leader的添加日志指令，并将日志记录添加至自己的日志中

## 快速恢复

- **日志恢复机制太慢**：上述日志恢复机制每次`NextIndex`减一，对实际场景太慢

- **额外信息**：若follower因为日志不一致而拒绝leader的添加日志指令，可携带额外信息加速日志恢复

  **ConflictTerm**: follower与leader冲突的日志记录的term，如果follower在对应位置没有log则设为-1

  **ConflictIndex**: follower的日志记录中任期号为ConflictTerm的第一条log的index，当follower在对应位置没有log时则返回log的长度

- **leader快速恢复**

  找到term等于ConflictTerm的log，若找到则将nextIndex设为该term的最后一个日志记录之后的位置；若没有找到则将nextIndex设为ConflictIndex

## 持久化

- **故障恢复机制**：当某些节点shutdown后，可通过启动另外一些服务器以替换shutdown的节点继续工作的方式，实现故障恢复
- **持久化存储状态信息**：但必须将某些状态信息持久化存储，从而可以恢复新启动的服务器的状态

- **需要持久化的状态信息**：votedFor、currentTerm、log

  持久化votedFor和currentTerm确保每个节点在某个任期内只给一个candidate投票，保证了任何时候只有一个leader

  持久化log是因为这是进行快速恢复状态的唯一信息

## 日志快照技术

- **日志占用空间**：如果任由日志持续增长，最终会导致磁盘空间被占满

- **根据日志恢复耗费时间**：如果每次重启服务器，都要从日志的第一条记录开始重新执行，则将耗费大量时间

- **日志快照**

  **snapshot条目**：应用程序将其状态的拷贝作为一种特殊的log条目（snapshot）存储下来

  **snapshot与log的大小**：大多数应用程序的状态大小远小于log的大小，因此将状态保存在快照中能节约空间

  **丢弃之前的日志记录**：当进行快照后，该快照之前的快照，以及该快照对应的最后一个index前的日志记录都可以被丢弃。恢复时只需要加载该快照的状态变量即可，无需从头开始执行各个日志记录

- **装载快照RPC**

  **follower落后过多**：当某些follower远落后于leader，以至于其nextIndex对应的日志记录已被leader丢弃

  **leader发送快照**：此时leader会发送装载快照RPC，并携带相应快照

  **follower装载快照**：follower收到该装载快照RPC则将其发送给状态机，并让他加载该快照中的状态，从而跟上leader当前状态

## 线性一致

- 如果一个服务表现得像只有一个服务器，且服务器没有故障，每次只执行一个客户端请求并正确返回，则认为该服务是线性一致的

