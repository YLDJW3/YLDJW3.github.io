---
title: 项目-6.824总结
date: 2022-03-10 16:20:34
tags: 
    - Distributed System
    - 面试
    - 项目
mathjax: true
categories: 项目
---

# Lab1 MapReduce

## 任务需求

## 关键问题分析

### RPC设计

- 需要设计两种类型的RPC
  - **请求任务RPC**：Workers向Coordinator请求任务，Coordinator向Workers返回其角色、任务
  - **任务完成RPC**：Worker告知Coordinator任务已完成，包含输出文件等信息，Coordinator返回Workers已收到

#### 请求任务RPC

```go
type TaskArgs struct {}

type TaskReply struct {
    Role int
    Index int
    NReduce int
    Files []string
}
```

- Role

  `Role=-1`代表返回错误，可能由于coordinator已经完成所有任务或coordinator故障

  `Role=0`代表当前未分配任务，worker应该在一段时间后再发送请求

  `Role=1`代表分配map任务

  `Role=2`代表分配reduce任务

- Files

  如果coordinator给worker分配了任务，则Files存储了worker需要读取的文件名，如果是map任务则为输入文件，如果是reduce任务则为map产生的中间值存储的文件

- NReduce

  **输出文件数量**：代表由客户指定的输出文件的数量，Map生成的中间值也会分为NReduce份，并存储在NReduce个中间值文件中

  **中间值分配**：采用`hash(key)%NReduce`的方法决定一个数据应该存储在哪个中间值文件

- Index

  标识worker的序号，对于Map worker，总共有M个，因此$1 \leq Index \leq M$；对于Reduce worker，总共有R个，因此$1 \leq Index \leq R$

#### 任务完成RPC

```go
type CompleteArgs struct {
	Files []int
	Role  int
	Index int
}

type CompleteReply struct {}
```

- Role

  worker告知master自己的身份，在请求任务RPC中由master分配

  `Role=1`代表Map worker

  `Role=2`代表Reduce worker

- Index

  worker的序号，在请求任务RPC中由master分配

- Files

  worker产生的结果文件的名字，若为Map worker则是中间值文件；若为Reduce worker则是输出文件

### Master需要维护的状态

- 保存**用户提供的参数**

  `files`，输入文件的名字

  `nReduce`，用户希望划分的输出文件数量

- **map任务的状态**

  Master根据用户提供的M个输入文件分配M个Map任务，每个Map任务都需要维护其是否完成的状态，**0标识未分配，1标识已分配，2标识已完成**

- **reduce任务的状态**

  Master根据用户提供的nReduce参数分配R个Reduce任务，每个Reduce任务都需要维护其是否完成的状态，**0标识未分配，1标识已分配，2标识已完成**

- map任务产生的**中间值文件的名字**

  Map worker通过任务完成RPC向master发送自己生成的中间值文件名字，master需要存储该中间值文件，以便分配给Reduce workers

### Master的行为

#### 初始化

- 保存用户提供的参数，初始化状态变量，并创建一个goroutine监听客户发送的RPC请求

#### 分配任务RPC handle

- map任务分配

  如果存在未分配的map任务，则进行分配并返回`Role=1`

  如果所有map任务已分配，但有map任务未完成，则返回`Role=0`，表示当前暂无新任务

- Reduce任务分配

  只有所有map任务都完成时，才分配Reduce任务

  如果存在未分配的reduce任务，则分配并返回`Role=2`

  如果所有reduce任务已分配，存在未完成的reduce任务，则返回`Role=0`，表示当前暂无新任务

- 如果所有任务都已经完成

  返回`Role=-1`，告知所有任务已完成

#### 任务完成RPC handle

- Map任务完成

  master查看任务状态

  - 若为0则表示此任务已经超时，忽略之
  - 若任务状态为1则表示处于分配状态，继续以下处理

  master在handle中存储中间值文件的名字，以便后续分配给reduce worker

  master更新map任务的状态为2，表示已完成

- Reduce任务完成

  master查看任务状态

  - 若为0则表示此任务已经超时，忽略之
  - 若任务状态为1则表示处于分配状态，继续以下处理

  master更新reduce任务的状态为2，表示已完成

  master保存输出文件的名字

#### **监听goroutine**

- **作用**：为了防止由于部分worker执行速度较慢导致的任务处理速度变慢，master为**每个分配的任务**都创建一个**监听goroutine**，该goroutine保证任务在超时间隔内未完成，将**重新分配给其他worker**

- **超时**：监听goroutine定期查看任务状态，若任务状态变为2，即已完成则退出。若任务状态始终为1，且时间达到了timeout，则将任务状态修改为0。这样分配任务RPC handler就会将该任务分配给之后请求任务的worker

#### 告知用户是否完成的Done方法

- MapReduce开放Done方法，供用户查询所有任务是否完成
- 检查每个map任务与reduce任务的状态，若都为已完成则返回true

### Worker的行为

- 在循环中不断地进行请求任务RPC
  - 若返回`Role=-1`，则说明任务都已完成或Master故障，退出程序
  - 若返回`Role=0`，则说明暂时无新任务，等待1s后重新请求
  - 若返回`Role=1`，则执行map任务
  - 若返回`Role=2`，则执行Reduce任务

#### Map worker

扫描每个一分配的输入文件

- **打开文件**：打开该文件（实际情况是，如果文件并非存储在本地的GFS，则需要向chunk server发送请求并接收文件）

- **map保存中间键值对**：map并保存中间键值对：对该文件中的所有数据执行map，并将产生的中间键值对保存（先将所有的）

- **根据哈希值分配**：创建R个中间值文件，并将每个中间键值对分配到R个中间值文件中的一个（根据`hash(key)%R`的值进行分配）

- **中间值文件的命名**：`mr-x-y`表示由第x个Map worker产生的第y个中间值文件，y的意思是该中间值文件应该由编号为y的Reduce worker进行处理

  > x由master分配的index决定
  >
  > y由该中间值的键经y = hash(key) % R求得

- **任务完成RPC**：当完成所有map任务后，则将产生的中间值文件名字列表，通过任务完成RPC的方式发送给master

#### Reduce worker

扫描每一个分配的中间值文件

- **打开文件**：打开该文件（实际情况是，如果中间值文件并非存储在本地的GFS，则需要向chunk server发送请求并接收文件）
- **读取所有键值对**：读取每个文件的键值对并保存在一个临时区
- **排序**：按键进行排序，则具有相同键的数据将排列在一起
- **扫描所有键值对并进行reduce**：扫描键值对时，如果下一个数据与其具有相同的键则一起处理，直至下一个数据的键不相同，然后将该项数据reduce并存入输出文件
- **输出文件的命名**：index为y的reduce worker的输出结果保存在输出文件`mr-out-y`

### 锁的使用

- master的RPC handler中需要修改共享数据结构，如中间文件名列表等，此时需要获取锁
- 读取数据时获取**共享锁**，修改数据时获取**排他锁**

## 成员函数实现

## 总结

- 实现了MapReduce框架中的Master节点与Worker节点的基本功能

  对于Master节点，实现的**基本功能**包括

  - 接收用户的请求，获取输入文件名字、输出文件数量等参数，并开始执行任务
  - 将Map划分为M份，并将M份map任务分配给map workers，map workers完成任务后告知master，包含其产生的中间文件名字。每个map worker可能产生最多R份中间文件
  - 待所有map任务完成后，开始执行reduce。将Reduce划分为R份，并将R份reduce任务分配给reduce workers，reduce workers完成任务后告知master，包含其产生的输出文件名字
  - 接收用户的查询请求，如果当前任务已经完成则返回true

  此外，Master节点还提供了一定程度的**负载均衡分配**

  - 假如一个Map worker或Reduce worker长期未返回（lab中超时时间为10s），则将其任务重新分配给其他空闲的workers

  Master节点与Worker节点的**通信**

  - 在实际场景中，Master节点与Worker节点位于不同地理位置，需要通过网络通信

  - lab中采用了RPC远程过程调用的方式实现Master节点向Worker节点分配任务，以及Worker节点告知Master节点任务执行结果

  - 具体地，设计了两种类型的RPC：请求任务RPC和任务完成RPC，均由worker主动调用，master中的RPC handle对相应的RPC进行处理

    请求任务RPC中，worker向master请求任务，master返回worker被分配的任务类型、worker编号、输入/中间文件的名字

    任务完成RPC中，worker向master告知完成，master接收worker传来的中间/输出文件的名字并保存

- 在基础上实现一个读取多文件并统计各个单词出现次数的引用

# Lab2 Raft

## Lab2A leader选举

### 任务需求

- **心跳信号**：实现leader节点的心跳信号，实现时可通过向其他节点发送不带有日志记录的**添加日志指令**实现
- **leader选举机制**：实现leader选举机制，包括
  - **选举计时器**：选举计时器的正确设置，每次超时后都需要随机选择一个超时时间，然后重启计时器，超时时间的上限为MBTF、下限为心跳信号间隔的数倍
  - **开始选举**：节点开始选举时将自己设为candidate，给自己投票，然后发送**选举请求RPC**给其他节点

### Raft规则

#### RequesetVote RPC

- 参数

  `term`：候选人的任期号

  `candidateId`：候选人的id

  `lastLogIndex`：候选人最后一个日志记录的index

  `lastLogTerm`：候选人最后一个日志记录的任期号

- 返回

  `term`：如果其他节点的任期号更新，则它会返回它的任期号，当前candidate则退化为follower并更新自己的任期号

  `voteGranted`：若为true则代表候选人收到了选票

- 其他节点中RPC处理函数的逻辑

  **拒绝**：如果自己的任期号更新，则返回false，并将任期号附在`term`

  **投票**：如果当前`votedFor`为空或是该候选人的id，而且候选人的日志记录至少和自己一样**up-to-date**，则向其投票

  > 回顾：up-to-date的概念
  >
  > - 对方的`lastLogTerm`更大
  > - `lastLogTerm`一样，而`lastLogIndex`更大或相等，则认为对方至少和自己一样up-to-date

#### 服务器行为

- **降级为follower**：一旦收到一个RPC请求或回复，传递了一个任期号大于自身的任期号，则将自身的任期号更新，并转换为folloewr

##### follower行为

- **回复**：回复leader或candidate发送的RPC
- **超时竞选**：一旦超时，则转换为candidate，并开始竞选

##### Candidate行为

- **竞选**：成为candidate后立刻开始竞选，增加任期号，为自己投票，并向其他节点发送请求投票RPC
- **统计选票**：收到其他服务器发送的RequestVote RPC的回复，并统计获得的选票
- **获得过半数选票**：一旦收到过半数选票，则变为leader身份
- **放弃竞选**：如果收到其他节点的添加日志RPC，说明已存在leader，竞选失败，变为follower
- **超时重新竞选**：如果定时器超时，仍然未获得过半数选票，且未收到其他节点的添加日志RPC以声明它们是leader，则重设定时器并重新开始竞选

##### Leader行为

- **发送心跳**：竞选成功后，立即向其他服务器都发送心跳信号（空的添加日志RPC）,以表明自己的leader身份

- **接收客户请求**：添加日志记录到本地的日志，并向其他服务器发送添加日志RPC，过半数节点确认后，提交日志，并执行该操作然后将结果返回给客户

- **发送添加日志指令**：如果某个服务器的`nextIndex`小于等于当前的日志最大index，则向其发送添加日志RPC，并附上从`nextIndex`开始的日志。如果服务器由于日志不一致而拒绝该请求，则递减该服务器的`nextIndex`并重新发送

- **提交日志**：如果有过半数的写入了某个日志，且该日志的index大于当前的commitIndex，且该日志的任期等于当前任期，则将这个新日志提交，并更新commitIndex

  > 从上述表述可知，假如该日志不是当前任期写入的，即使过半数的节点添加了该日志，仍然不能进行提交

### 关键问题分析

#### 超时间隔的选取

- **心跳信号间隔**：心跳信号的间隔选择为100ms
- **超时间隔**：超时时间的间隔选择为300-600ms
- **随机种子**：每次随机选择超时间隔时，设置一个随机种子，可使用时间作为随机种子，但需要注意`time.Now().Unix()`返回的是**秒**，应该使用`time.Now().UnixNano()`返回的**纳秒**作为随机种子

#### 选举计时器的行为

- 什么时候重启计时器？

  **收到心跳**：收到来自其他leader的有效的心跳信号时

  **收到选票请求**：其他candidate发送选票请求RPC，并接收请求向其投票时

  **超时**：此时需要成为candidate并开始新的选举，同时随机选择超时间隔，然后重启计时器

#### 参与竞选时的行为

- **状态变量**

  维护`votedFor`代表当前任期内向哪个节点投票

  维护`receiveVotes`代表如果参加了竞选，获得了多少选票

- **超时**：若选举计时器超时，则参与竞选，角色设置为candidate

- **为自己投票**：竞选者设置`votedFor`和`receiveVotes`

- **并行拉票**：向其他节点发送投票请求RPC，每个节点都应该**并行地向其他节点发送投票请求RPC**，即创建多个goroutine，每个goroutine向某个节点发送投票请求RPC

  ```go
  for i := 0; i < len(rf.peers); i++ {
      if i == rf.me {
          continue
      }
      // send RequestVote in parallel
      go rf.helperRequestVotes(i, &args)
  }
  ```

#### 节点退化为follower

- **退化为follower**：任何时候，只要节点收到了一个选票请求RPC或添加指令RPC，其任期号term大于自己当前的任期号，则节点退化为follower

### 总结

- **bug-串行发送RPC**：习惯性地将candidate发送RequestVote RPC的行为写在for循环之中，从而导致它们是串行发送的，如此导致等待RPC返回的时间加起来，几乎一定超过Election timer的timeout，则无法产生稳定的leader

  解决：candidate发送`RequestVote`，以及leader发送`AppendEntries`都应该是**并行**的，切勿在循环中串行发送，否则

## Lab2B log

### 功能需求

- 实现leader和follower添加日志的功能

### Raft规则

#### AppendEntries RPC

- 参数

  `term`：leader的任期号

  `leadId`: leader的id

  `prevLogIndex`：新添加日志记录的上一个日志记录的index

  `prevLogTerm`: 新添加日志记录的上一个日志记录的term

  `entries[]`：日志记录数组

  `leaderCommitIndex`：leader的提交index，表示该index及其之前的日志记录都已提交

- 返回

  `term`：如果follower的任期号更大，则返回给leader让它更新

  `success`: 表示follower所拥有的日志记录与`prevLogIndex`和`prevLogTerm`相符，并成功添加leader所发送的日志记录

- 接收方规则

  **更新term**：如果`term`比自己当前的任期号更小，则leader应该更新，返回false

  **日志不一致**：如果`prevLogIndex`位置上不存在日志记录或者该日志记录的任期号与`prevLogTerm`不相符，则返回false

  **删除冲突日志记录**：如果已存在的日志记录与新的日志记录不一致，则将其及之后的日志记录都删除

  **添加新日志**：添加leader发送的日志记录`entries`

  **提交日志**：如果`leaderCommitIndex`大于自己的`commitIndex`，则尽心更新，将`commitIndex`更新为`leaderCommitIndex`与当前拥有的最新日志记录的index的较小者

### 关键问题分析

#### 是否需要额外的SendHeartBeat RPC？

- 并不需要，因为SendHeartBeat心跳信号只是为了让follower知道当前leader处于正常工作状态，这可以通过AppendEntries RPC实现。且AppendEntries在需要时应附上发送的数据，否则只是发送一个空的AppendEntries RPC即可

#### 日志快速回复是怎样实现的？

- **follower返回额外信息**

  **ConflictTerm**: follower与leader冲突的日志记录的term，如果follower在对应位置没有log则设为-1

  **ConflictIndex**: follower的日志记录中任期号为ConflictTerm的第一条log的index，当follower在对应位置没有log时则返回log的长度

- **leader快速恢复**

  找到term等于ConflictTerm的log，若找到则将nextIndex设为该term的最后一个日志记录之后的位置；若没有找到则将nextIndex设为ConflictIndex

## Lab2C persistence

### 功能需求

- **故障恢复机制**：当某些节点shutdown后，可通过启动另外一些服务器以替换shutdown的节点继续工作的方式，实现故障恢复
- **持久化存储状态信息**：但必须将某些状态信息持久化存储，从而可以恢复新启动的服务器的状态

- **需要持久化的状态信息**：votedFor、currentTerm、log

  - 持久化`votedFor`和`currentTerm`确保每个节点在某个任期内**只给一个candidate投票**，保证了任何时候只有一个leader

  - 持久化`log`是因为这是进行快速恢复状态的唯一信息

### Raft规则

#### 需要持续化存储的状态

- **实时写入稳定存储器**：以下状态一旦变化，则应该写入稳定存储器

- `currentTerm`：从0开始，不断递增
- `votedFor`：当前任期内该节点向谁投票了
- `log[]`：保存了日志记录，每个**日志记录**都包含需要state machine执行的**指令**，以及从leader收到该日志记录时的**任期号**

#### 存储在内存的状态

- `commitIndex`：当前最后一条提交的日志记录的序号
- `lastApplied`：当前最后一条应用在状态机上的日志记录的序号

#### leader需要存储在内存的状态

- `nextIndex[]`: 对每个服务器都会一个`nextIndex`，代表leader需要发送给follower让其添加的第一条日志记录的序号
- `matchIndex[]`：每个服务器与leader匹配的的日志记录中最后一条的序号
- **初始化**：以上两个状态会在选举成为leader后重新初始化，`nextIndex`初始化为当前leader日志记录的最后一条的序号+1，`matchIndex`初始化为0

## Lab2D 日志压缩

### 功能需求

- **日志占用空间**：如果任由日志持续增长，最终会导致磁盘空间被占满

- **根据日志恢复耗费时间**：如果每次重启服务器，都要从日志的第一条记录开始重新执行，则将耗费大量时间

- **日志快照功能**：为此，需要实现日志快照功能

  `Snapshot()`：上层应用调用`Snapshot()`告知Raft自己当前的状态，Raft用该状态生成一个快照，并丢弃该快照对应的index之前的所有日志记录清除

  `CondInstallSnapshot()`：Raft收到leader发送的一个快照后会通过`ApplyMsg`发送给上层应用。如果上层应用装载了该快照，则通过`CondInstallSnapshot()`告知raft自己装载了快照，因此raft可以将该快照对应的index之前的所有日志记录清除

  `InstallSnapshot RPC`：当leader发现follower的日志落后太多时，它会调用`InstallSnapshot RPC`，让follower装载快照，以跟上自己的日志进度

  

## Lab2 实现具体解析

### Make()

- **初始化**：主要负责初始化各个状态变量，包括从存储空间中读取持久化的状态变量，读取快照并恢复状态等
- **开始竞选**：在一个goroutine中开始ticker()函数，它将实现竞选计时器，并在计时器中断时重启，然后开始竞选
- **开始应用操作**：在另一个goroutine中开始Apply()函数，主要根据当前CommitIndex判断某个日志记录是否已提交，若提交则将该操作应用在state machine上

### ticker()

- **`timerReset`变量**：根据raft原理，在收到leader的心跳信号，或者收到其他candidate的requestVote时，都会重启计时器，在实现时我们不会直接重启，而是设置`timerReset`变量为true

- **定时器计时行为**

  **随机选择超时间隔**：首先每次定时都会随机选取一个超时间隔（300-600ms），然后Sleep以等待该间隔

  **在循环中每次Sleep10ms**：实现时，为了监控重启计时器的行为，我们不会直接Sleep超时间隔，而是在循环中每次Sleep10ms

  **检查是否重启计时器**：然后唤醒并检查是否重启了计时器（`timerReset`是否为true），如果为true则跳出循环并重启计时器（重新选择超时间隔，然后循环中Sleep）

- **开始竞选**

  - **超时**：如果在循环Sleep过程中一直没有重启计时器，且循环次数达到设定值，则发生了超时。此时需要

  - **变为candidate**：设置`Roles`为candidate

  - **为自己投票**：`votedFor`设为自己的id，`receivedVotes`设为1

  - **并行发送RequestVote RPC**：**立刻并行地**（创建多个goroutine，每个goroutine运行`helperRequestVotes`，负责向某一个服务器发送）向其他服务器发送`RequestVote RPC`，且带上自己的`LastLogIndex`和`LastLogTerm`等信息

### helperRequestVotes()

- **功能**：该函数负责发送RequestVote RPC，并根据返回的参数做出相应 的操作

- **发送RPC**：通过系统提供的接口`sendRequestVote`发送RequesetVote RPC

- **返回处理**

  **降级**：如果返回了更大的任期号，则从candidate降级为follower

  **获得选票**：如果返回投票，则更新`receivedVotes`

  **获得超过半数选票**：升级为leader，创建goroutine并执行`sendHeartbeat`发送心跳信号，初始化各个server的`nextIndex`为当前自己日志记录中的下一个index，初始化各个server的`matchIndex`为0

### SendHeartbeat()

- **确认leader身份**：若并非leader则返回
- **并行发送空的AppendEntries RPC**：对每个其他server，创建一个goroutine并执行`helperSendAppendEntries`
- **等待100ms**：Sleep心跳信号间隔时间100ms后，开始下一次循环

### helperSendAppendEntries()

- **功能**：AppendEntries RPC包含两个功能，让follower**添加日志的指令**，同时也作为**心跳信号**让follower知道leader正在正常工作

- **确认leader身份**：若非leader则返回

- **设置RPC参数并发送**：任期号、leader id、prevLogIndex、PrevLogTerm、Entries、LeaderCommit等

- **返回处理**

  **降级**：如果返回了更大的任期号，则从candidate降级为follower

  **心跳信号**：则设置心跳信号发送成功标志`successHeartBeats[server]`

  **添加日志成功**：更新该follower的`nextIndex`和`matchIndex`

  **提交日志**：如果超过半数的节点的`matchIndex>=N`，且第N个日志记录的任期号是当前leader的任期号，且N对应的日志记录未提交，则提交N以及之前的所有未提交日志记录

  > 实现中，由于我们在一个独立的goroutine运行了Apply()程序，它将持续检测commitIndex，如果大于当前已提交的日志记录，则讲新的日志记录提交并让state machine执行。因此，我们只需要更新commitIndex即可，而且自动就是累积提交形式

  **添加日志失败**：假如follower由于日志不一致，拒绝了我们的添加日志指令，此时我们只需要递减`nextIndex`并再次发送请求即可

  **加速日志恢复**：如果每次都只是递减`nextIndex`则速度太慢，可以由follower在返回中附带额外信息，加速日志恢复。follower会在返回中携带`ConflictIndex`和`ConflictTerm`，意为冲突日志记录的任期号，以及follower中该任期号的第一条日志记录的index。leader收到后，将在自己的日志记录中查找`ConflictTerm`，找到了则将`nextIndex`设为该任期最后一条日志记录的下一位置，没找到则将`nextIndex`设为`ConflictIndex`

### RequestVote()

- **功能**：响应其他candidate发送的RequestVote RPC

- **candidate过时**：如果当前term比candidate传过来的term更新，则拒绝该请求，并让candidate更新它的term
- **更新自己的term**：如果当前term小于candidate传过来的term，则更新自己的term，并转换为follower
- **投票**：如果当前任期内还未投票，而且candidate的log至少跟自己一样up-to-date，则进行投票
- **重启计时器**：由于收到了其他candidate的请求投票RPC，因此需要重启计时器，设置`timerReset`

### AppendEntries()

- **功能**：响应leader发送的AppendEntries RPC的handle
- **检查过时**：如果发送方任期号小于自己任期号，返回false
- **检查日志一致性**：如果`prevLogIndex`没有日志记录，或者该日志记录的`prevLogTerm`不符，则返回false并携带相应信息
- **添加日志**：如果检查通过，则将leader发送过来的日志记录添加到自己的日志中，假如自己的日志中相应位置已有记录，则先将他们切除再添加
- **提交日志**：如果leader的commitIndex比自己的commitIndex打，则更新，goroutine中运行的Apply会将相应的日志记录提交给state machine执行
- **重设定时器**：因为收到了leader的AppendEntries RPC，应该重设计时器

### Snapshot()

- **功能**：上层应用将当前状态告知raft，请求其创建快照并压缩log

- **检查`index`是否更新**：如果上层应用发送的状态的`index`比之前快照的`lastIncludedIndex`更新则创建快照，否则忽略
- **保存快照**：如果更新则将快照保存在`rf.snapshot`
- **压缩日志**：将`index`及之前的日志记录都删除，并将其写入稳定存储器（持久化）

### CondInstallSnapshot()

- **功能**：上层应用状态机希望加载某个快照，它通过此函数询问raft是否加载
- **检查该index后是否进行提交**：如果raft的`commitIndex`比该快照的`lastIncludedIndex`更新，则在该快照后raft已提交了新的日志记录，不能加载该快照
- **保存快照并压缩日志**：如果该快照比当前raft的快照更新，则保存在`rf.snapshot`，同时修建日志，写入稳定存储器（持久化）
- **返回true让上层应用加载该快照**：返回true代表上层应用状态机可以加载快照

### helperSendInstallSnapshot()

- **功能**：leader向follower发送该RPC，让其加载快照以跟上自己的日志进度

- **参数**

  `term`：leader的任期号

  `leaderId`：leader的id

  `lastIncludedIndex`：这个快照包含了这个序号前的所有日志记录

  `lastIncludedTerm`：这个快照包含的最后一个日志记录的任期号

  `data[]`：快照数据内容

- **返回**

  `term`：如果返回了一个更新的任期号，则更新自己的任期号并变为follower

### InstallSnapshot()

- **功能**：处理上述`InstallSnapshot RPC`
- **leader有效检测**：如果leader的任期号小于自己，则该leader已经过时，返回任期号告知其进行更新
- **重启定时器**：由于收到leader的有效信息，因此重启定时器
- **检测snapshot是否更新**：如果`lastIncludedIndex`大于自己的，则该snapshot更新，将其通过ApplyMsg传递给上层应用

### Apply()

- **功能**：不断检测commitIndex以判断raft是否提交新日志记录，若提交则将其应用于state machine
- **检测新提交日志记录**：在循环中（sleep 10ms）不断比较commitIndex与lastApplied
- **将日志应用于上层应用**：如果有新日志记录则调用`sendApplyMsg()`将该应用操作发送给上层应用

### sendApplyMsg()

- **功能**：将序号为index的日志记录中的操作提交给上层应用让其执行

- **发送ApplyMsg**：设置ApplyMsg中的Command、CommandValid、CommandIndex、LeaderId等参数，然后将其送入applych

  ```go
  rf.mu.Lock()
  msg := ApplyMsg{}
  msg.Command = rf.logs[rf.GetLogIndexWithLock(index)].Command
  msg.CommandValid = true
  msg.CommandIndex = index
  msg.LeaderId = rf.leaderId
  rf.mu.Unlock()
  rf.applyCh <- msg
  ```

### Start()

- **功能**：接收上层应用提交的操作，若为leader则往日志中添加一条该操作的日志记录，并向其他节点都发送添加日志指令
- **若自己并非leader**：只有leader可以添加日志记录，如果并非leader，则返回false同时将当前的leaderId告知客户
- **若自己为leader**：若为leader则添加日志记录，同时向其他节点都发送添加日志指令，该**函数应该立即向客户返回true**。如果此后该操作的日志记录提交，raft会通过SendApplyMsg的形式告知客户操作已提交。上层客户会监听通道上的ApplyMsg并获取到该消息，从而将操作应用在state machine上

# Lab3 K/V service

## Lab3A K/V service without snapshots

### 功能需求

- **Clerk中的客户接口**：客户调用Clerk接口执行Put/Append/Get操作，Clerk通过相应的RPC调用向服务器发送请求执行Put/Append/Get操作
- **服务器RPC handle**：客户调用RPC请求执行Put/Append/Get操作，服务器中相应的RPC handle进行处理。它将该操作通过`start`发送给raft，并不断监听ApplyMsg消息以获取该操作是否提交，若提交则将操作应用在state machine上，并将结果返回给客户
- **去重**：客户可能向多个服务器发送相同的操作请求，state machine最多执行一次，因此需要唯一标识客户的操作

### raft规则

#### GetArgs

- **功能**：用户发送Get操作请求时的参数
- `Key`：查询的键
- `ClientId`：客户id
- `CommandId`：指令id

#### GetReply

- **功能**：服务器返回告知Get操作的结果
- `Err`：错误码，包含OK（执行成功）、ErrWrongLeader（当前服务器非leader）、ErrExpired（请求已过期）、ErrTimeout（超时）错误码

- `Value`：若查询成功，则返回`Key`对应的值
- `LeaderHint`：若当前server非leader，它可以在此变量存放当前leader的id

#### PutAppendArgs

- **功能**：客户端向服务器请求Get/Append时的参数
- `Key`：Put或Append的键
- `Value`：Put或Append的值
- `ClientId`：客户id
- `CommandId`：指令id
- `Op`：指示操作类型为Get或Append

#### PutAppendReply

- **功能**：服务器向客户端返回Get/Append的执行结果
- `Err`：错误提示码，含义同上
- `LeaderHint`：若服务器非leader则返回当前leader id

### 关键问题分析

#### 为什么客户的操作可能重复执行？

**客户对每个请求都设置最大等待时间**：因为客户对每个申请的reply都设置有一定的超时时间（1s），它不可能一直阻塞，一旦超时了它就会向其他server再次申请该操作

**网络延迟等导致raft leader提交日志的时间过长**：假如某个raft leader由于网络延迟等原因，从添加日志到操作日志的时间间隔过长，则可能无法及时向用户返回该reply，但实际上该操作已经被apply在state machine上

**该操作已被apply，而客户又再次发起了该操作**：为此，必须使用session结构记录某个`clientId`客户当前已执行的最后操作的id`commandId`，若收到小于等于该id的操作则应该忽略，保证不重复执行

#### 如何保证客户操作不重复执行？

**唯一标识**：通过clientId + commandId的方式，对客户操作进行唯一标识，将一个command应用在state machine的同时，还需要记录该客户当前应用的上一个commandId

**去重**：如果应用一个消息时发现`op.CommandId <= kv.session[op.ClientId].CommandId`，则说明该操作已执行，无需重复执行，直接返回其保存的结果`kv.session[op.ClientId].Response`即可

**start前去重与执行前去重**：我们在将操作发送给raft前执行一次查重，若重复则无需让raft添加该日志记录。此外，收到raft返回的提交记录后，我们调用`Execute()`执行该操作前也进行一次查重，若重复则无需再次执行

### 具体实现

#### 客户端

##### Get()/PutAppend()

- **传入Get/Put/Append参数**：填写GetArgs/PutAppendArgs中的clientId、commandId、Key、Value等参数
- **向服务器发送Get/Put/Append RPC**
- **可能的返回结果**
  - **该操作的日志记录已提交**：则获取操作结果Value即可
  - **该服务器并非leader**：取`LeaderHint`返回值作为下一次请求的服务器对象，若`LeaderHint`值无效则随机选择一台服务器
  - **超时/过期**：随机选择 一台服务器作为下一次请求的目的地

#### 服务端

##### Get()/PutAppend()

- **功能**：接收客户端发送的Get/PutAppend操作请求，并发送给raft，让其添加并提交一条包含该操作的日志记录

- **查重**：在将操作发送给raft前，先检查对应clientId当前的commandId，若该操作已经执行，则直接返回结果

- **调用Start将操作发送给raft**

  ```go
  index, _, isleader := kv.rf.Start(op)
  ```

- **可能的返回结果**

  **该raft节点并非leader**：则返回客户相应的leaderHint

  **提交成功**：该raft节点是leader，则它会添加日志并发送给其他follower，此时我们需要监听raft发送的Apply消息

  **监听raft的ApplyMsg**

  - **成功收到applyMsg**：若收到raft发送的apply消息，则将其与所发送的Get/PutAppend操作进行对比，如果clientId和commandId都相符，则说明请求的操作已被raft提交，返回结果给用户

  - **超时**：等待一定时间后仍未收到消息，则告知客户超时，让其重新请求操作

##### Apply()

- **监听applyCh**：如果raft提交了某个日志记录，则会通过applyCh告知state machine，因此我们在一个goroutine上执行`Apply()`持续监听raft发送的ApplyMsg
- **收到操作提交的消息并执行**：调用`Execute()`执行该操作

- **将结果发送给Get()/PutAppend()**：服务器的Get/PutAppend运行在某个goroutine并一直监听其通道上的执行成功消息，因此执行操作后向该通道发送一个消息，携带相应的操作内容与执行结果

##### Execute()

- **查重**：执行前根据该操作指令的commandId和clientId确认其是否重复
- **执行并更新session**：若该操作未执行则执行，同时更新session，然后返回结果

## Lab3B Key/Value service with snapshots

### 功能需求

- **检查日志记大小**：因为raft的日志记录需要持久化存储，若日志记录持续增长最终将导致空间耗尽。为此上层应用会定期检查日志空间，若超过一定大小则利用当前state machine的当前状态生成一个快照，并发送给raft，请求它进行日志快照

- **快照**：客户通过`Snapshot()`和`CondInstallSnapshot()`与raft交互，并进行快照操作

### 具体实现

#### CheckSnapshot()

- **监视日志大小**：在一个goroutine中，（sleep 10ms）循环监视当前的`persister.RaftStateSize()`是否超过了`masraftstate`
- **生成snapshot并发送给raft**：若超过了则将当前的状态变量保存，包含当前的键值对关系和去重用的`session`，生成snapshot后调用`rf.Snapshot()`发送给raft

#### Apply()中监听snapshot

- **监听snapshot消息**：raft一旦提交某个操作，会将该操作相应的ApplyMsg发送给上层应用。此外，raft装载了某个快照，也会将该快照通过ApplyMsg发送给上层应用
- **调用`CondInstallSnapshot`确认快照可装载**：如果raft在发送Snapshot的ApplyMsg后，又提交了新的操作，则不可以装载该快照，否则可能造成覆盖。因此调用`CondInstallSnapshot`进行确认
- **装载快照**：将当前的键值对数据`data`和去重数据`session`替换为快照中的值

# Lab4 Shard K/V Service

## Lab4A The Shard Controller

### 关键问题分析

- **Multi-raft**：一个raft集群里只有一个leader节点，客户请求任何操作都必须与该leader节点进行通信，因此吞吐量较低。为此增加若干个raft集群共同提供服务，客户可以与其中某个raft的leader进行通信，从而增加吞吐量
- **multi-raft管理**：当前有多个raft群，对于K/V服务，规定每个raft集群都负责某一部分键值对存储。因此我们需要一个mutli-raft控制器，去管理当前系统中的多个raft集群，并保存各个raft集群管理哪部分K/V的信息。常用的方法可以是将具有键`key`的键值划分为第`i=hash(key)%N`个分片，每个raft集群管理哪些分片则由controller负责管理

- **multi-raft控制器**：如上所述，整个系统只设置一个multi-raft控制器，为了保证容错性，控制器也必须运行在一个raft集群上。同时，它能够处理其他raft集群的请求，包括

  **Join**：raft集群**加入**系统

  **Leave**：raft集群**退出**系统）

  **Move**：分片**迁移**，将某个分片从集群A移动到集群B

  **Query**：某个集群**查询**当前自己管理的分片

- **负载均衡原则**：为了尽量在多个raft集群间平衡负载，我们按键值将数据分为N个分片，保证每个raft集群负责的分片数的差不超过1

## Lab4B Sharded Key/Value Server

### 关键问题分析

#### raft集群获取config更新信息

- **config**：指的是哪个集群负责哪些分片的信息
- **获取并更新config**：控制器不会主动发送消息告知raft集群config更新，因此raft集群需要在一个goroutine中不断地调用Query获取当前最新的config更新

#### raft集群进行分片迁移

- **分片迁移**：由于config更新，可能导致分片迁移，分为两种情况，要么是之前不属于自己的分片被分配给自己管理，要么是之前属于自己的分片被分配给自己管理。

- **分片状态**：因此对于每个raft集群，其分片状态包含以下四种可能

  - `NotServed`：该分片不属于自己管理
  - `Serving`：该分片正在由自己管理
  - `Pulling`：该分片正在进行迁移，从其他raft集群获取中
  - `Pushing`：该分片正在进行迁移，推送给其他raft集群，在推送完成前不能清除该分片的数据

- 分片迁移时需要**传输的数据**

  - 当前保存的**键值**

  - 查重数据**session**

- **分片迁移需要经过raft进行**：分片迁移时我们可能会往data或session插入数据，也可能删除数据，为了保证一致性，分片迁移操作都应该向raft的leader节点发送请求，raft提交该日志后再进行apply
