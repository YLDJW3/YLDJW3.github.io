---
title: 6.824 lab 4
date: 2021-12-23 16:40:07
tags: 6.824 
categories: Distributed System
---

# Part A

- pass

# Part B

```go
for i in {0..100}; do go test -race; done
```

## 2021-12-25

- Client端的leader不从Server中获取，而是直接在Client端记录

- 将Operation（Put/Append/Get）与Migration（ConfigNum/ReadMigration/WriteMigration）分离，有各自的数据类型、Execute函数、消息channel等

- Test Snapshot问题描述：Client Get出现了大量的**WrongLeader**，导致了时间大量浪费

  - 经仔细检查，大量WrongLeader是由于**Client中的leader赋值错误导致的**

  ```go
  if ok && (reply.Err == OK || reply.Err == ErrNoKey) {
      // 正确赋值
      ck.leader[gid] = (si + ck.leader[gid]) % len(servers)
      // 之前的错误赋值
      ck.leader[gid] = si
      return reply.Value
  }
  ```

  - 十一次Test Snapshot出现了一次Get错误，成功的平均时间为27s

- 问题描述：server端无法更新`kv.leaderHint`
  - 若检测到config更新而自己并非leader，**尝试提交empty op**

- 问题描述，提交empty op的确能解决以上问题，但是会导致前三个test时间非常长
  - 可能原因是因为**empty op不应该在`checkConfig`中提交**，否则会导致每个服务器都向ctrler查询最新的config并向raft提交empty op，代价太大
  - 解决：在**一个单独的协程中调用GetState()**即可
- test4无法稳定通过，可能存在的问题：
  - 活锁？一直ErrWaiting
  - 有一个shard一直是2
  - Get错误等
  - **修改：**Get和PutAppend的准入条件
- 问题描述：在某个时刻某个shard在所有group的状态都是Waiting

<!--more-->

## 2021-12-26

- ReadMigration回复的条件修改

  - 若当前的configNum小于read的configNum，则数据未更新，需要等待
  - 若当前的configNum等于read的configNum，则分两种情况，若该shard处于等待状态，说明其本质上还是上一个config，若非等待状态，则可以读取
  - 若当前的configNum大于read的configNum，则可以read
  
  ```go
  // check if the shard is in waiting status
  // if kv.shardState[args.Index] == Waiting {
  if kv.configNum < args.ConfigNum || (kv.configNum == args.ConfigNum && kv.shardState[args.Index] == Waiting) {
      kv.mu.Unlock()
      reply.Err = ErrWaiting
      return
  }
  ```
  
- 结果：t1-4用时稳定在50多秒，但t3/t4有接近20%的几率无法通过，表现为**Get的结果缺少了某一个Append的数据**

- 问题描述：`TestConcurrent`出现了活锁的问题
  - 问题检查：ReadMigration长期ErrTimeout，大量的ReadMigration堆积在Raft中
  - 解决一：对每个shard设置一个ReadMigration专用的锁，只有获取了锁才可以发送ReadMigration RPC
  - 解决二：在ReadMigration RPC handler中增加对重复Migration的检查，若session中有则直接返回
  - 结果：TestConcurrent能在10s左右通过，但可能出现**goroutine数目过多、Get不正确、活锁**的报错，需要考虑进一步考虑`SendReadMigrationRPC`的限制（现在的锁只能让其阻塞，但是仍然会创建该goroutine）

- 问题描述：`TestConcurrent`出现了ConfigNum无法更新的活锁的问题

  - 解决：在向Raft提交ConfigNum更新时增加对重复Migration的检查，若session中有则直接返回

  - 解决：因为假如ConfigNum到达乱序，则会导致该操作被查重拒绝，因此必须增加标志位flag，如果ConfigNum则将flag置为false，不要保存到session中

  ```go
  if mg.Operation == "ConfigNum" {
      DPrintf1("mg.ConfigNum %v, kv.configNum %v", mg.ConfigNum, kv.configNum)
      if mg.ConfigNum == kv.configNum+1 {
          config := kv.mck.Query(mg.ConfigNum)
          lastConfig := kv.mck.Query(mg.ConfigNum - 1)
          // update config and setup the shard state, which will start the shard migration process
          for i := 0; i < len(kv.shardState); i++ {
              if lastConfig.Shards[i] == kv.gid && config.Shards[i] != kv.gid {
                  kv.shardState[i] = NotServed
              } else if lastConfig.Shards[i] != kv.gid && config.Shards[i] == kv.gid {
                  if kv.configNum == 0 {
                      kv.shardState[i] = Serving
                  } else {
                      kv.shardState[i] = Waiting
                      // go kv.SendReadMigrationRPC(i)
                  }
              }
          }
          kv.configNum = mg.ConfigNum
      } else {
          flag = false
      }
      entry.Data = nil
  }
  if flag {
      kv.migrationSession[mg.Gid][mg.Index][mg.Operation] = entry
  }
  ```

  - 结果TestConcurrent不会卡死，但是出现了几次**Get的结果缺少最近一次Append的情况**

## 2021-12-27

- 问题描述：TestConcurrent成功时时间约为12s，但是会出现**Get的结果缺少最近一次Append的情况**
  - 分析：怀疑是Operation的判断条件出现问题，在Concurrent的情况下暴露出来

- 问题描述：TestUnreliable出现Get的结果多了一个Append的情况
  - 分析：同上，可能是**Get/PutAppend的允许条件有问题**

- 问题描述：Migration后Operation可能重复执行，由于session不一致的原因
  - 解决方法：Migration时候考虑将该shard的data和session一起转移
  - 修改：data目前是按shard的，**同样地，将session也按shard划分**，在migration时（包括ReadMigration和WriteMigration）相关的数据结构除了data还要带上session

- 问题描述：修改后仍然存在不一致的情况
  - 分析：可能是各个map在传递时没有采用复制的形式
  - 修改：migration时，data和session都使用专门编写的copy函数进行复制，同时从migrationSession中获取data和session也使用copy的形式
  - 结果：Unreliable仍出现Get结果的问题，大部分表现为**缺少某段前缀**，而重复Append的问题似乎得到了解决
  - 重新测试Test1-4：20次通过

- 问题描述：本问题来自于网上看到，如果snapshot大小为0则应该直接跳过而不应该尝试解码

  ```go
  snapshot := persister.ReadSnapshot()
  if snapshot == nil || len(snapshot) < 1 { // bootstrap without any state?
      DPrintf("Error empty")
  } else {
      r := bytes.NewBuffer(snapshot)
      d := labgob.NewDecoder(r)
      var data []map[string]string
      var opSession []map[int64]OpEntry
      var migrationSession map[int][]map[string]MigrationEntry
      var shardState []int
      var configNum int
      if d.Decode(&data) != nil || d.Decode(&opSession) != nil || d.Decode(&migrationSession) != nil || d.Decode(&configNum) != nil || d.Decode(&shardState) != nil {
          // error...
      } else {
          kv.data = data
          kv.opSession = opSession
          kv.migrationSession = migrationSession
          kv.configNum = configNum
          kv.shardState = shardState
      }
  }
  ```

- 在WriteMigration执行时，对比版本号`ConfigNum`

  ```go
  if mg.Operation == "WriteMigration" {
      if mg.ConfigNum == kv.configNum && kv.shardState[mg.Index] == Waiting {
          kv.shardState[mg.Index] = Serving
          kv.data[mg.Index] = kv.CopyMap(mg.Data)
          kv.opSession[mg.Index] = kv.CopyMap1(mg.Session)
          entry.Data = nil
          entry.Session = nil
      } else {
          flag = false
      }
  }
  ```

  

- 问题描述：通过对Unreliable的log查找，发现在更新ConfigNum时，会用到LastConfig和config的对比，但有时候LastConfig获取的结果可能是最新的config

  - 问题分析：这个问题较为异常，因为lastConfig系通过`mg.ConfigNum - 1`获取的，而config正常说明`mg.ConfigNum - 1`不应该小于0或超过当前最新configNum，因此不应该出现LastConfig的情况，**怀疑是Controller的问题**
  - 解决：根本上的解决可能涉及Controller的修改，目前通过获取LastConfig和config后，重新对比两者的ConfigNum暂时解决

  ```go
  config := kv.mck.Query(mg.ConfigNum)
  lastConfig := kv.mck.Query(mg.ConfigNum - 1)
  if config.Num == mg.ConfigNum && lastConfig.Num == mg.ConfigNum-1 {
      // update config and setup the shard state, which will start the shard migration process
      for i := 0; i < len(kv.shardState); i++ {
          if config.Shards[i] != kv.gid {
              kv.shardState[i] = NotServed
          } else if lastConfig.Shards[i] != kv.gid && config.Shards[i] == kv.gid {
              if kv.configNum == 0 {
                  kv.shardState[i] = Serving
              } else {
                  kv.shardState[i] = Waiting
                  // go kv.SendReadMigrationRPC(i)
              }
          }
      }
      DPrintf1("group %v, lastConfig is %v, this is %v, my state is %v", kv.gid, lastConfig, config, kv.shardState)
      kv.configNum = mg.ConfigNum
  } else {
      flag = false
  }
  ```

- SendReadMigrationRPC中可能有同样的问题，进行相同处理

  ```go
  lastConfig := kv.mck.Query(kv.configNum - 1)
  if lastConfig.Num != kv.configNum-1 {
      kv.mu.Unlock()
      kv.readMigrationMu[i].Unlock()
      return
  }
  ```

  

- 问题描述：多余的SendReadMigration

  - 解决：为了避免创建多余的协程，为每个shard的SendReadMigration设置一个`readMigrationMu`，同时为每个shard设置一个`CheckMigration(i int)`，在调用`SendReadMigrationRPC`前必须获取`readMigrationMu`，从而避免了之前创建过多goroutine的问题
  - 代码：注意到此时`SendReadMigrationRPC`函数应直接调用，不要创建goroutine

  ```go
  func (kv *ShardKV) CheckMigration(i int) {
  	for !kv.killed() {
  		kv.mu.Lock()
  		_, isleader := kv.rf.GetState()
  		configNum := kv.configNum
  		kv.mu.Unlock()
  		if isleader {
  			kv.readMigrationMu[i].Lock()
  			kv.SendReadMigrationRPC(i, configNum)
  			kv.readMigrationMu[i].Unlock()
  		}
  		time.Sleep(time.Millisecond * 50)
  	}
  }
  ```

- 问题描述：ReadMigration得到了空的map
  - 修改：检查代码怀疑是ReadMigration RPC handler中的执行前查重器，由于其data和session的赋值没有使用编写的copy函数，因此进行了修改，执行前查重器仍然保留
  - 结果：出现了**Get缺少最近一次Append的情况**

- WriteMigration不要直接覆盖

  ```go
  // kv.data[mg.Index] = kv.CopyMap(mg.Data)
  // kv.opSession[mg.Index] = kv.CopyMap1(mg.Session)
  for k, v := range mg.Data {
      kv.data[mg.Index][k] = v
  }
  for k, v := range mg.Session {
      kv.opSession[mg.Index][k] = kv.CopyEntry(v)
  }
  ```

  - 结果：Unreliable50次测试有1次出错，**Get缺少了一段前缀**

## 2021-12-28

- 问题描述：执行用时过长，TestConcurrent3用时超过了120s

  - 修改：考虑到Raft的执行顺序，尝试将状态从Waiting变为Serving的更改提前到Start(WriteMigration)后，而不是执行WriteMigration时
  - 结果：会出错，撤销修改

  ```go
  if mg.Operation == "WriteMigration" {
      if mg.ConfigNum == kv.configNum {
          // kv.shardState[mg.Index] = Serving
          for k, v := range mg.Data {
              kv.data[mg.Index][k] = v
          }
          for k, v := range mg.Session {
              kv.opSession[mg.Index][k] = kv.CopyEntry(v)
          }
          entry.Data = nil
          entry.Session = nil
          DPrintf1("group %v writeMigration, configNum %v, data %v", kv.gid, kv.configNum, mg.Data)
      } else {
          flag = false
      }
  } 
  ```

- 问题描述：各个RPC handler在**第一时间检查是否leader**，若不是则直接回绝

  ```go
  _, isleader := kv.rf.GetState()
  if !isleader {
      reply.Err = ErrWrongLeader1
      kv.mu.Unlock()
      return
  }
  ```

- 问题描述：耗时过长

  - 一个可能的原因，SendReadMigrationRPC需要获取该shard的锁，如果该程序耗时过长，则可能导致性能大幅降低
  - 检查发现checkMigration的bug，**没有预先检查Waiting状态再调用`SendReadMigrationRPC`**，修改后

  ```go
  if isleader && kv.shardState[i] == Waiting {
      configNum := kv.configNum
      kv.mu.Unlock()
      kv.readMigrationMu[i].Lock()
      kv.SendReadMigrationRPC(i, configNum)
      kv.readMigrationMu[i].Unlock()
  } 
  ```

  - 服务端在进行Migration时长时间出现的WrongLeader，考虑对服务端也**增加上一个leader的存储**

- 目前状态：大概率通过所有测试，小概率出错表现在**TestConcurrent2的Get缺少某次Append的结果**。另外**耗时过长**，其中TestConcurrent3可能超过规定的120s，还需要进一步解决时间问题
  - 解决思路：由于Get结果出错概率小，复现需要测试次数多，考虑**先解决耗时问题**，再查bug

- 问题描述：由于**TestConcurrent2的Get缺少某次Append的结果出现概率较高**，因此考虑先解决该问题
  - 观察log，难以直接确认，感觉是WriteMigration覆盖了之前的Append结果。由此联想到之前WriteMigration采取了追加而不是覆盖的方式，是否应该直接覆盖？
  - 如何处理**Migration与Operation之间的关系**

## 2021-12-29

- 对于Get和PutAppend的读写也需要进行Serving校验

  ```go
  if reflect.TypeOf(msg.Command).String() == "shardkv.Op" {
      // DPrintf1("Operation command")
      op := msg.Command.(Op)
      em := OpMsg{}
      if kv.shardState[key2shard(op.Key)] == Serving {
          res := kv.ExecuteWithLock(op)
          kv.applyIndex = msg.CommandIndex
          em.op = op
          em.res = res
          em.err = OK
      } else {
          em.err = ErrWrongGroup
      }
      ch, ok := kv.channels1[msg.CommandIndex]
      if !ok {
          kv.mu.Unlock()
          continue
      }
      // send ExecuteMsg to RPC handler througn go channel should be placed in goroutine
      go kv.SendApplyOpMsg(ch, em)
      kv.mu.Unlock()
  }
  ```

- **如果一个Group的某个shard等待被拉取，那么它也不能提供读写服务或者更新config，增加状态Pushing表示正在等待其他group发来ReadMigration**

## 2021-12-30

- 问题描述：状态从Pushing改成NotServed时各个服务器之间不同步，有的服务器已经变为NotServed，有的却一直在Pushing

  - 分析：因为**Migration可能存在执行不成功的情况**，此时应该返回ErrWaiting，而不是OK

  - 修改：根据`MigrationWithLock`的返回值flag判断是否成功，若不成功则设置`em.err = ErrWaiting`

    ```go
    flag, data, session := kv.MigrationWithLock(mg)
    em := MigrationMsg{}
    ch, ok := kv.channels2[msg.CommandIndex]
    DPrintf2("Migration ApplyMsg %v received", msg.CommandIndex)
    if !ok {
        DPrintf2("Nobody listen to the migration msg, type %v", msg.Command.(Migration).Operation)
        kv.mu.Unlock()
        continue
    }
    if flag {
        kv.applyIndex = msg.CommandIndex
        em.migration = mg
        em.data = data
        em.session = session
        em.err = OK
    } else {
        em.err = ErrWaiting
    }
    ```

  - 结果：问题仍然存在，考虑**对状态为Pushing的shard增加InstallMigration RPC**

- 问题描述：group重启后由于snapshot没有保存最新的状态，可能卡在上一个config的Pushing状态，而实际上相应的其他gruop读取了该shard数据并进入下一个config，因此之后也不会有gruop来读取该数据，从而使得状态一直被阻塞在Pushing

  - 解决：增加InstallMigrationRPC主动推送数据，从而pushing和pulling两者并列存在

  - 协调：由于Install的设置主要是为了协助解决Pushing阻塞问题，主要的功能已经由之前的ReadMigration实现。因此InstallMigration RPC handler需要特别注意以下几种情况

    - `kv.configNum > args.ConfigNum`，说明当前gruop已经具有该数据，无需重复安装，直接返回OK
    - `kv.configNum == args.ConfigNum && kv.shardState[args.Index] != Pulling`，同上，返回OK
    - `kv.configNum < args.ConfigNum`，说明当前group未更新到该config，返回ErrWaiting

    因此，只有当前group的config与之相等且shard的状态为Pulling时才进行更新

  - **InstallMigration RPC的调用者收到OK的回复后，向Raft commit一个NotServed Migration，用以将相应的shardState从Pushing设置为NotServed** 

  - 测试结果：TestConcurrent3出现了Get缺少某次Append的错误

- 问题描述：ReadMigration阻塞

  - 修改：对于**configNum小于自己的ReadMigration**，无论当前状态，都可以允许操作

  ```go
  // check if the shard is in waiting status
  if kv.configNum < args.ConfigNum || (kv.shardState[args.Index] != Pushing && kv.configNum == args.ConfigNum) {
  // previous condition, which is wrong
  // if kv.configNum < args.ConfigNum || (kv.shardState[args.Index] != Pushing) {
      kv.mu.Unlock()
      reply.Err = ErrWaiting
      return
  }
  ```

  - 测试结果：59/60，其中第55次出现了TestConcurrent3缺少某段的情况，考虑对TestConcurrent3进行单独测试

- 问题描述：TestConcurrent3的Get缺少了一段
  - 测试结果：连续200次的第79次出错，608438行

## 2021-12-31

- 发现WriteMigration覆盖了之前的数据，分析发现WriteMigration的config不应该使用当前的`kv.configNum`，而应该使用发送ReadMigration时候的configNum，`args.configNum`

  ```go
  func (kv *ShardKV) WriteMigration(index int, data map[string]string, session map[int64]OpEntry, configNum int) {
  	kv.mu.Lock()
  	// check if that WriteMigration has been commited
  	_, ok := kv.migrationSession[kv.gid]
  	if ok && kv.migrationSession[kv.gid][index]["WriteMigration"].ConfigNum >= kv.configNum {
  		kv.mu.Unlock()
  		return
  	} else {
  		mg := Migration{}
  		mg.Gid = kv.gid
  		mg.ConfigNum = configNum
  		mg.Operation = "WriteMigration"
  		mg.Index = index
  		mg.Data = data
  		mg.Session = session
  		_, isleader := kv.rf.GetState()
  		if isleader {
  			kv.mu.Unlock()
  			kv.rf.Start(mg)
  			kv.mu.Lock()
  		}
  		kv.mu.Unlock()
  	}
  }
  ```

- 测试结果：连续190次pass
- 连续对Concurrent3测试1000次pass
